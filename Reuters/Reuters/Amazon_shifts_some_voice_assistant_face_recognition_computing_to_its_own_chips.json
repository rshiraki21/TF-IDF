{"text": "When users of devices such as Amazons Echo line of smart speakers ask the voice assistant a question, the query is sent to one of Amazons data centers for several steps of processing. When Amazons computers spit out an answer, that reply is in a text format that must be translated into audible speech for the voice assistant.Amazon previously handled that computing using chips from Nvidia but now the majority of it will happen using its own Inferentia computing chip. First announced in 2018, the Amazon chip is custom designed to speed up large volumes of machine learning tasks such as translating text to speech or recognizing images.But major technology companies are increasingly ditching traditional silicon providers to design their own chips. Apple on Tuesday introduced its first Mac computers with its own central processors, moving away from Intel chips. [nL1N2HW1WF}Amazon said the shift to the Infertia chip for some of its Alexa work has resulted in 25% better latency, which is a measure of speed, at a 30% lower cost.Amazon has also said that Rekognition, its cloud-based facial recognition service, has started to adopt its own Inferentia chips. However, the company did not say which chips the facial recognition service had previously used or how much of the work had shifted to its own chips.The service has come under scrutiny from civil rights groups because of its use by law enforcement. Amazon in June put a one-year moratorium its use by police after the killing of George Floyd."}